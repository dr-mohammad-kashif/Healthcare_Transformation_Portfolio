# Why do Small Data Errors Matter in Health Care Analytics

Smaller mistakes in health care data do not generally draw attention to themselves; they typically involve a small detail such as a missing value or an unclearly labeled variable. However, small details like this are rarely confined to themselves alone and over time accumulate into large-scale distortions of the information used for decision-making.

The missing age variable is a great example of how common small data errors can have significant effects on analyses. Although age is commonly included as a variable in many clinical models, age is used to identify and stratify risks for patients. Therefore, when age values are either missing or are inconsistent across a dataset, the overall view of the data will be distorted. Additionally, vulnerability in older populations may be underestimated (or undervalued) and trends identified through the use of broad population categories may be incorrectly applied to populations that do not share the characteristics of the population being trended.

In addition to missing variables, follow-up indicators also provide another type of error that can occur when using a database. A number of databases contain a very simple yes/no indicator of whether or not a follow-up contact has been made with a patient. This indicator appears to be quite clear-cut at first glance. In fact, without additional information, it can be challenging to determine what "yes" means regarding the follow-up contact. Does "yes" indicate that a contact was made with the patient? Did an appointment get scheduled? Did the patient actually attend an appointment? With the absence of contextual information, this variable can be challenging to interpret. Nevertheless, this indicator is often used to evaluate the continuity of care provided to patients; measure the amount of work each provider has; and/or provide input into resource allocation decisions. The underlying assumption is that the data accurately represents the real world. Unfortunately, all too often this does not occur.

The way providers are named in a database also contributes to the distortions discussed above. Depending on the individual entering the data or the database from which the data originated, the same physician may have multiple names. For instance, Dr. Smith may be listed as "Smith, J." in one database and "John Smith, MD" in another. When these types of issues occur, the accuracy of performance metrics and the ability to link performance metrics to specific physicians or departments may become questionable. Similarly, apparent differences in physician practices may not reflect actual differences in practices but rather differences in data entry â€” a difference that is not easily explained after scorecards have already been released.

In essence, the underlying premise of these issues is the concept of trust. Executives and clinicians generally assume that dashboards have been developed based on solid foundation(s). A clean summary reinforces that assumption. However, the clean summary does not disclose the missing values, the inconsistent coding, or the fields that have changed meanings depending on which contributor entered the field. The gap between the appearance of being a reliable source of information and the actual reliability of the data is where risks come into play.

It's not about having perfect data. There will never be a time when a health care dataset is completed. Waiting for flawless data will result in no action. But analytics still needs to mean something. If the information gleaned cannot be relied upon as a basis for decision making related to a patient's care, it should not be used. Data quality is not merely a "back office" issue or a "nice-to-have" technical feature; it is fundamental to any analytics platform. If data quality is compromised then everything that is developed upon it also is compromised.
